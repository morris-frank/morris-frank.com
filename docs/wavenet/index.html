<!DOCTYPE html><html><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#a9384a">
    <meta name="referrer" content="no-referrer">

    <link rel="stylesheet" href="https://morris-frank.dev/main-41efeedfed.css">
    <link rel="shortcut icon" type="image/png" href="https://morris-frank.dev//icon.png">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

    <title>WaveNet from scratch | morris-frank.dev</title>
</head>

<body>
    <nav class="center">
        <h1>
            <a class="no-decorate" href="https://morris-frank.dev/">Morris Frank</a>
        </h1>
        <a href="https://morris-frank.dev/">blog</a>
        <a href="https://morris-frank.dev/resume.pdf">resume</a>
    </nav>

    <main class="center">
        <h2>WaveNet from scratch</h2>
        <script type="text/javascript" src="https://morris-frank.dev/figures/wavenet.js"></script>

        <p>
            Today we are building a WaveNet <a href="#oordWaveNet2016">[1]</a> from
            scratch. The WaveNet is an <em>autoregressive</em>, <em>generative</em> and
            <em>deep</em> model for audio signals.
        </p>

        <p>
            `x &gt; y`
        </p>

        <p>
            <em>This guide is geared towards readers with a background in modern
                deep learning.</em>
        </p>

        <div class="tags">
            <span>PyTorch</span><span>Deep Learning</span><span>Sound processing</span>
            <span>Generative modelling</span>
        </div>

        <hr>

        <h3>The problem</h3>
        <p>
            Lets say we want to generate sound signals. This might happen in different
            settings. We might have music notes + a instrument and want to generate
            their sound \(w(\mathrm{note}, \mathrm{instrument}) = \mathrm{sound}\), or we
            have a text and want to generate the corresponding speech
            \(w(\mathrm{text}) = \mathrm{sound}\) or maybe we already have a sound signal,
            but it is noisy and we want to remove the noise \(w(\mathrm{noisy\ sound}) = \mathrm{sound}\).

            For all of these we need to find a model \(w(\cdot)\) which can generate
            high-quality semantic sounds.
        </p>

        <p>
            Sound is just a 1-dimensional temporal signal. Therefore the the same methods
            can be used for other similar signals like <a href="https://w.wiki/DHK">EEG</a>
            or financial data. I will ignore those areas here but you can find cool
            applications in other literature.
        </p>

        <h3>Using Waveforms</h3>
        <p>
            Sounds are vibrations of air, increase and decrease of air pressure over time.
            As such sound, measured at one given point, is a 1D-signal over time. Stereo
            sound recordings (over more than that) are multiple such signals, from different
            spatial positions. When we are working with sound digitally we need an
            approximation of those recorded analog signals. The simplest digitalization is
            <a href="https://w.wiki/DHJ">Pulse-code modulation</a> (PCM). In PCM we take
            samples from the analog signal and discretize them at fixed same-length
            intervals. For e.g. 8kHz 16bit PCM that means that we take a sample from the
            vibration every \(\frac{1}{8000}\mathrm{sec}\) and choose the closest of
            \(2^{16}=65536\) bins/amplitude values. Below we have a small excerpt of a sound
            file encoded with 16kHz 16bit.
        </p>

        <figure>
            <img src="https://morris-frank.dev/img/wave.jpg">
        </figure>

        <p>
            As we have here a sample length of 100 at a sample rate of 16kHz this sample is
            <em>only</em> 6.25 ms long. Directly we see the problem in working with music in
            the time-domain: Interesting information is at completely different temporal
            scales. On the low level we have the characteristics of different notes, e.g.
            a piano has frequencies between around 30 Hz to 4 kHz, on top of you could
            expect modulating effects (e.g a voice vibrato is around 6Hz), growing larger in
            the temporal dependencies, a short rhythm in the range of seconds to the
            structure of songs / works at multiple minutes. How can we capture all those
            scales with one model?
        </p>

        <p>
            The general idea of the WaveNet to have a <em>causal</em> generation process.
            This means that each predicted output value is only based on information of
            previous input values, given an ordering. Now the idea of this causal
            autoregressive process comes from previous works by the same authors (and other)
            PixelRNN <a href="#oordPixelRNN2016">[2]</a>, and incorporating convolutions
            into the same process in PixelCNN <a href="#oordPixelCNN2016">[3]</a>. In those
            they also used the same principle to generate images, quite successfully.
            Naturally they took this to sound, as sound, in contrast to images, actually has
            an natural 1D-ordering.
        </p>

        <p>
            For the WaveNet the input is assumed to be as long as the output that we want to
            generate. For now lets assume we have a sound to sound translation task, so that
            the input and output are of the same type (Mono-audio).
        </p>

        <figure>
            <svg id="figcausal_svg" height="300" width="600">
                Sorry, your browser does not support inline SVG.
            </svg>
            <script type="text/javascript" src="https://morris-frank.dev/figures/causal.js" defer=""></script>
        </figure>

        <h3>Dilated Convolutions</h3>
        <figure>
            <svg id="figdilated_svg" height="300" width="600">
                Sorry, your browser does not support inline SVG.
            </svg>
            <script type="text/javascript" src="https://morris-frank.dev/figures/dilated.js" defer=""></script>
        </figure>

        <p>
            Do make the dilated convolutions we are doing the dilation and the convolution
            separately. Looking back to the animation above, we see that for the dilation in
            the first hidden layer the convolution we kind of have two <em>dense</em>
            convolutions (with the same kernel), for the even and the odd elements. In the
            implementation we can achieve that by splitting up the input along its time axis
            and transposing as such that the these blocks go into the batch-size dimension.
        </p>

        <p>
            In PyTorch we always have the dimensions ordering \(\mathrm{batch\_size} \times \mathrm{channels} \times \mathrm{length}\).
        </p>

        <p>
            So for an input of the size \(4\times 1\times 128\) with a dilation of \(2\) we
            end up with \(8\times 1\times 64\). Or a direct example (\(2\times 1\times 4\)):
        </p>

        \[
        \begin{align*}
        x &amp;= \begin{bmatrix}0 &amp; 1 &amp; 2 &amp; 3\\A &amp; B &amp; C &amp; D\end{bmatrix}\\
        &amp;\Rightarrow\\
        dilate(x, 2) &amp;= \begin{bmatrix}0 &amp; 2\\A &amp; C\\1 &amp; 3\\B &amp; D\end{bmatrix}
        \end{align*}
        \]

        <p>
            Of course now we have the difficulty that our batch dimension is cluttered with
            all blocks from the different samples in the mini-batch. Therefore we always
            need to keep track of the dilation we are having right now, so that we can
            reverse it. In code this gives us:
        </p>

        <pre class=" language-python"><code class=" language-python"><span class="token keyword">def</span> <span class="token function">dilate</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> new<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> old<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    :param x: The input Tensor
    :param new: The new dilation we want
    :param old: The dilation x already has
    """</span>
    <span class="token punctuation">[</span>N<span class="token punctuation">,</span> C<span class="token punctuation">,</span> L<span class="token punctuation">]</span> <span class="token operator">=</span> x<span class="token punctuation">.</span>shape  <span class="token comment"># N == Batch size × old</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>dilation <span class="token punctuation">:</span><span class="token operator">=</span> new <span class="token operator">/</span> old<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> x
    L<span class="token punctuation">,</span> N <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>L <span class="token operator">/</span> dilation<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>N <span class="token operator">*</span> dilation<span class="token punctuation">)</span>
    x <span class="token operator">=</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    x <span class="token operator">=</span> torch<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">[</span>C<span class="token punctuation">,</span> L<span class="token punctuation">,</span> N<span class="token punctuation">]</span><span class="token punctuation">)</span>
    x <span class="token operator">=</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> x<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>

        <p>
            Now that we dilated we can apply a normal 1-dim convolution on the new Tensor
            which will go along the time axis and thus have a dilated receptive field. For
            the animations given above we would dilate three times each with a factor of two
            (\(2, 4, 8\)).
        </p>

        <p>
            <i>As a sidenote: This is different from the dilated (à trous) convolution as
                implemented in PyTorch's <samp>nn.Conv1d</samp> itself.
            </i>
        </p>

        <h3>Gates, Residuals and Skips</h3>
        <p>
            The different hidden layers in the model have differently sized receptive fields
            (play around with the animation to see this). We introduced the problem of
            time-domain modelling as a problem of different temporal scales. Now these
            different layers, so the assumption, will look at features at different scales,
            combining the information from more low-level features. As the inference process
            needs the information from all those levels, the model employs skip-connections.
            Instead of taking the output of the last layer the actual model output is the
            sum of projections out of all the layers.
        </p>
        <p>
            Next we see that not all information flowing upwards from the low-level to the
            high level is useful information to keep, especially with the big temporal
            receptive field the flow of information needs to be regulated. Here he authors
            take the idea of gated convolutions as it is known from e.g. LSTM
            <a href="hochreiterLong1997">[5]</a>. For each hidden layer we have have two
            convolutions followed by a <samp>sigmoid</samp> \(\sigma\) and a <samp>tanh</samp>,
            respectively. The <samp>sigmoid</samp> gives a scaling in \([0, 1]\) and is
            acting as the gate (the value being the amount of allowed information to flow).
            The <samp>tanh</samp> gives a scaling of \([-1,1]\) and is acting as the feature
            magnitude. Their outputs are just multiplied, to <i>apply the gating</i>. Keep
            in mind though that this does not necessarily accurately predict the trained
            behavior, but it has shown better training performance in comparable settings.
        </p>
        <p>
            Further all hidden layers are constructed as residuals, the to the convolution
            is added back to the output of the convolutions. Residual learning ensures that
            the zero-centered initialization of the weights constructs an identity mapping,
            meaning if the layer does not learn anything it also does not degrade the
            inference in any way <a href="heDeep2015">[6]</a>. For deeper models this has
            shown to considerably improves training speed.
        </p>
        <p>
            To summarize the construction of one hidden layer: The output of the previous
            layer gets dilated, we keep this as the reference, goes through the gated
            convolution giving the residual. The residual is than added to the
            skip-connection flow and to the reference as the input for the next hidden
            layer.<br>
            In a picture:
        </p>

        <figure>
            <img src="https://morris-frank.dev/figures/wavenet_dilated_block.svg">
        </figure>

        <p>
            Why the \(1\times 1\) convolutions after the residual? The width (channels) of
            the residual, skip and reference might be different. Therefore we need to learn
            a channels to channels mapping, which is precisely a \(1\times 1\) convolution.
        </p>

        <p>
            Setting the forward pass of one hidden layer in code we will get something
            along:
        </p>

        <pre class=" language-python"><code class=" language-python">dilated <span class="token operator">=</span> dilate<span class="token punctuation">(</span>feat<span class="token punctuation">,</span> new<span class="token operator">=</span>new_dil<span class="token punctuation">,</span> old<span class="token operator">=</span>old_dil<span class="token punctuation">)</span>

filters <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>filter_conv<span class="token punctuation">(</span>dilated<span class="token punctuation">)</span><span class="token punctuation">)</span>
gates <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>gate_conv<span class="token punctuation">(</span>dilated<span class="token punctuation">)</span><span class="token punctuation">)</span>
residual <span class="token operator">=</span> filters <span class="token operator">*</span> gates

feat <span class="token operator">=</span> dilated <span class="token operator">+</span> feat_conv<span class="token punctuation">(</span>residual<span class="token punctuation">)</span>
skip <span class="token operator">=</span> skip <span class="token operator">+</span> skip_conv<span class="token punctuation">(</span>residual<span class="token punctuation">)</span>
</code></pre>

        <h3>Putting it together</h3>

        <p>
            In the <samp>__init__</samp> we have to prepare all the layers:
        </p>

        <pre class=" language-python"><code class=" language-python"><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_blocks<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">,</span> n_layers<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">,</span>
                feat_width<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">32</span><span class="token punctuation">,</span> residual_width<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">32</span><span class="token punctuation">,</span>
                skip_width<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">32</span><span class="token punctuation">,</span> kernel_size<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">,</span>
                bias<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token builtin">super</span><span class="token punctuation">(</span>WaveNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>n_blocks<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_layers <span class="token operator">=</span> n_blocks<span class="token punctuation">,</span> n_layers
    self<span class="token punctuation">.</span>bias <span class="token operator">=</span> bias

    self<span class="token punctuation">.</span>filter_conv <span class="token operator">=</span> self<span class="token punctuation">.</span>_conv_list<span class="token punctuation">(</span>feat_width<span class="token punctuation">,</span> residual_width<span class="token punctuation">,</span>
                                        kernel_size<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>gate_conv <span class="token operator">=</span> self<span class="token punctuation">.</span>_conv_list<span class="token punctuation">(</span>feat_width<span class="token punctuation">,</span> residual_width<span class="token punctuation">,</span>
                                        kernel_size<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>skip_conv <span class="token operator">=</span> self<span class="token punctuation">.</span>_conv_list<span class="token punctuation">(</span>residual_width<span class="token punctuation">,</span> skip_width<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>feat_conv <span class="token operator">=</span> self<span class="token punctuation">.</span>_conv_list<span class="token punctuation">(</span>residual_width<span class="token punctuation">,</span> feat_width<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
</code></pre>

        <p>
            With a helper function to generate all the list of convolutions:
        </p>

        <pre class=" language-python"><code class=" language-python"><span class="token keyword">def</span> <span class="token function">_conv_list</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> out_channels<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
                   kernel_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Gives a list of convolutions of length blocks × layers.
        :param in_channels:
        :param out_channels:
        :param kernel_size:
        :return:
        """</span>
        module_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_blocks <span class="token operator">*</span> self<span class="token punctuation">.</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            module_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span>
                                         bias<span class="token operator">=</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>module_list<span class="token punctuation">)</span>
</code></pre>

        <p>
            And our forward pass:
        </p>

        <pre class=" language-python"><code class=" language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_blocks <span class="token operator">*</span> self<span class="token punctuation">.</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dilated <span class="token operator">=</span> dilate<span class="token punctuation">(</span>feat<span class="token punctuation">,</span> new<span class="token operator">=</span>new_dil<span class="token punctuation">,</span> old<span class="token operator">=</span>old_dil<span class="token punctuation">)</span>

        f <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>filter_conv<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>dilated<span class="token punctuation">)</span><span class="token punctuation">)</span>
        g <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>self<span class="token punctuation">.</span>gate_conv<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>dilated<span class="token punctuation">)</span><span class="token punctuation">)</span>
        residual <span class="token operator">=</span> f <span class="token operator">*</span> g

        feat <span class="token operator">=</span> dilated <span class="token operator">+</span> self<span class="token punctuation">.</span>feat_conv<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>residual<span class="token punctuation">)</span>
        skip <span class="token operator">=</span> skip <span class="token operator">+</span> self<span class="token punctuation">.</span>skip_conv<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>residual<span class="token punctuation">)</span>
</code></pre>

        <h3>Faster WaveNet</h3>


        <hr>

        <h3>References</h3>
        <ol class="references">
            <li>
                <cite id="oordWaveNet2016">
                    <span class="title">WaveNet: A Generative Model for Raw Audio</span>
                    <br>
                    <span>A. van den Oord et al., Sep. 2016</span>
                    <br>
                    <a href="https://arxiv.org/abs/1609.03499">arXiv:1609.03499</a>
                </cite>
            </li>
            <li>
                <cite id="oordPixelRNN2016">
                    <span class="title">Pixel Recurrent Neural Networks</span>
                    <br>
                    <span>A. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu, Aug. 2016</span>
                    <br>
                    <a href="https://arxiv.org/abs/1601.06759">arXiv:1601.06759</a>
                </cite>
            </li>
            <li>
                <cite id="oordPixelCNN2016">
                    <span class="title">Conditional Image Generation with PixelCNN Decoders</span>
                    <br>
                    <span>A. van den Oord, N. Kalchbrenner, O. Vinyals, L. Espeholt, A. Graves, and K. Kavukcuoglu, Jun. 2016</span>
                    <br>
                    <a href="https://arxiv.org/abs/1606.05328">arXiv:1606.05328</a>
                </cite>
            </li>
            <li>
                <cite id="paineFast2016">
                    <span class="title">Fast Wavenet Generation Algorithm</span>
                    <br>
                    <span>T. L. Paine et al., Nov. 2016</span>
                    <br>
                    <a href="http://arxiv.org/abs/1611.09482">arxiv:1611.09482</a>
                </cite>
            </li>
            <li>
                <cite id="hochreiterLong1997">
                    <span class="title">Long Short-Term Memory</span>
                    <br>
                    <span>S. Hochreiter and J. Schmidhuber, Neural Computation, vol. 9, no. 8, Nov. 1997.</span>
                    <br>
                </cite>
            </li>
            <li>
                <cite id="heDeep2015">
                    <span class="title">Deep Residual Learning for Image Recognition</span>
                    <br>
                    <span>K. He, X. Zhang, S. Ren, and J. Sun, Dec. 2015</span>
                    <br>
                    <a href="http://arxiv.org/abs/1512.03385">arXiv:1512.03385</a>
                </cite>
            </li>
        </ol>

    </main>

    <footer>
        <div class="center box flex">
            <div>
                <h2>Responsible person here</h2>
                <p><a href="mailto:maurice.frank@posteo.de">Maurice Frank</a></p>
                <p><a href="https://github.com/morris-frank/morris-frank.dev">source code (MIT)</a></p>
            </div>
            <div>
                <h2>Privacy statement</h2>
                <p>I don't collect any data from you. Neither do I want any data. So don't send me any!</p>
            </div>
        </div>
    </footer>


</body></html>